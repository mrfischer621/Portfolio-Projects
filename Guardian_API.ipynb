{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import datetime as dt\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1 - extract the information about Justin Trudeau since 01.01.2018\n",
    "\n",
    "url = 'https://content.guardianapis.com/search'\n",
    "search_term = 'Justin Trudeau'\n",
    "date_today = date.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "params = {'api-key':'8eb7b724-c4d4-42f0-845a-0617f9495526',\n",
    "            'page-size':50, #Page Size 100 or 200 caused an error.\n",
    "            'q':search_term,\n",
    "            'from-date': \"2018-01-01\",\n",
    "            'to-date': date_today,\n",
    "            'page': 1\n",
    "        }\n",
    "\n",
    "def query_api(url, params):\n",
    "    \"\"\"\n",
    "    Function to query the Guardian API with the preffered search term. \n",
    "    The output is a int with the amount of results and a string with the actual date and the amount of results to verify.\n",
    "    \"\"\"\n",
    "    response = requests.get(url, params)\n",
    "    return response.json()\n",
    "\n",
    "json_result = query_api(url, params)\n",
    "print('On the ' + date_today + ', we found ' + str(json_result['response']['total']) + ' results with the search term ' + search_term +'.')\n",
    "\n",
    "def query_all_articles(url, params):\n",
    "    \"\"\"\n",
    "    Function to iterate trough all the results/pages from the query and add the results to one dataframe.\n",
    "    The function adds at the params dict \"page\" in every iteration +1, to go trough every page automated. \n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(columns=['id', 'type', 'sectionId', 'sectionName', 'webPublicationDate', 'webTitle', 'webUrl', 'apiUrl','fields','isHosted','pillarId','pillarName'])\n",
    "\n",
    "    if int(json_result['response']['total']) > 200:\n",
    "        for i in range(0, (int((json_result['response']['total']) / int(params['page-size']) + 1))):\n",
    "            print('Iterate through page number ' + str(params['page'])) #To check the iterations\n",
    "            data = pd.DataFrame(query_api(url, params)['response']['results'])\n",
    "            df = pd.concat([df, data])\n",
    "            params['page'] = int(params['page']) + 1\n",
    "            time.sleep(3)\n",
    "                \n",
    "\n",
    "    return df\n",
    "\n",
    "df_guardian = query_all_articles(url, params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check & clean the data\n",
    "df_guardian.info() \n",
    "\n",
    "# 8212 rows, 12 columns\n",
    "df_guardian.shape\n",
    "\n",
    "# All datatypes are objects --> I wil lchange the 'webPublicationDate\" to datetime format and set as index for a easier data handling\n",
    "df_guardian.dtypes\n",
    "\n",
    "df_guardian['webPublicationDate'] = pd.to_datetime(df_guardian['webPublicationDate']).dt.strftime(\"%Y-%m-%d\")\n",
    "df_guardian = df_guardian.set_index('webPublicationDate')\n",
    "\n",
    "# I have found 20 row duplicates --> I will delete it\n",
    "df_guardian[df_guardian.duplicated()].count()\n",
    "df_guardian.drop_duplicates(inplace = True)\n",
    "\n",
    "# I have found 55 duplicates with the same webTitle --> I will delete it, beacuse the most of them are \"Correction and clarification\" or Football preview articles\n",
    "dup_webtitle = df_guardian.loc[:, 'webTitle'][df_guardian.loc[:, 'webTitle'].duplicated()]\n",
    "df_guardian.drop_duplicates(subset=['webTitle'], inplace = True)\n",
    "\n",
    "# I have found 8265 NaN Values in the column \"field\" --> I don't need this column, I will delete it\n",
    "df_guardian.isna().sum()\n",
    "\n",
    "# I have found 8265 Null Values in the column \"field\" --> I don't need this column, I will delete it\n",
    "df_guardian.isnull().sum()\n",
    "\n",
    "# For the tasks, I need just the columns type, sectionID, sectionName, webPucblication, webTitle\n",
    "df_cleaned = df_guardian.loc[:, ['type', 'sectionId', 'sectionName', 'webTitle']]\n",
    "\n",
    "df_cleaned.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 2 - Count how many articles about Justin Trudeau have been posted since 01.01.2018 until today (The data is already in this date range), grouped by Date and \"No. of articles\".\n",
    "\n",
    "# Check how many of the types are articles\n",
    "pd.crosstab(index = df_cleaned.loc[:, 'type'],\n",
    "           columns = 'count')\n",
    "\n",
    "# Set mask for type \"article\"\n",
    "mask_article = df_cleaned.iloc[:, 0] == 'article' \n",
    "\n",
    "# Define df with the type 'article' and the date between 01.01.2018 - today\n",
    "df_article = df_cleaned.loc[mask_article, :]\n",
    "\n",
    "\n",
    "# RESULT\n",
    "\n",
    "# Create table with \"Date\" and \"No. of articles\"\n",
    "df_date = pd.crosstab(df_article.index, columns = 'No. of article', rownames=[''], colnames=['Date'])\n",
    "df_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 3 - Calculate the average no of articles for all days for the above mentiod periods from \"No. of articles\"\n",
    "\n",
    "# RESULT\n",
    "\n",
    "# Total daily average since 01.01.2018 until today (Solution: 4 Articles per day, rounded)\n",
    "df_date.mean().round(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 4 - In which section (I choose 'sectionName') are the most articles written\n",
    "\n",
    "# RESULT\n",
    "\n",
    "# Define Dataframe with the sections\n",
    "df_section = pd.crosstab(index = df_article.iloc[:, 2],\n",
    "                columns = 'No. of article', \n",
    "                colnames = ['Section'], \n",
    "                rownames = ['']\n",
    ")\n",
    "\n",
    "# Show sections sorted by No. of articles (Solution: The \"World news\" section has the most articles with 1010)\n",
    "df_section.sort_values(by = ['No. of article'], ascending = False).iloc[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 5 - Show the evolution of the 'No. of articles? since 01.01.2018 until today\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# Had to change the index and add some date columns for a easier handling\n",
    "df_plot = df_date.reset_index()\n",
    "df_plot = df_plot.rename_axis(None, axis=1)\n",
    "df_plot['day'] = pd.to_datetime(df_plot['']).dt.to_period('D').dt.strftime('%d')\n",
    "df_plot['month-year'] = pd.to_datetime(df_plot['']).dt.to_period('M').dt.strftime('%Y %m')\n",
    "df_plot['month'] = pd.to_datetime(df_plot['']).dt.to_period('M').dt.strftime('%m')\n",
    "df_plot['year'] = pd.to_datetime(df_plot['']).dt.to_period('Y').dt.strftime('%Y')\n",
    "\n",
    "# Plot the data\n",
    "fig, ax = plt.subplots(nrows = 2, ncols = 1, figsize = (30, 15))\n",
    "sns.lineplot(x='month-year', y='No. of article', data=df_plot, ax=ax[0], alpha=0.8)\n",
    "sns.lineplot(x='month', y='No. of article', data=df_plot, ax=ax[1], alpha=0.8)\n",
    "\n",
    "# Style\n",
    "plt.tight_layout()\n",
    "ax[0].xaxis.set_tick_params(labelrotation=25)\n",
    "ax[1].xaxis.set_tick_params(labelrotation=25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 6 - Are there any unusual events in the time series under investigation?\n",
    "\n",
    "# Yes, I can see in the visualisation a higher amount of articles around 06/09 2018 and in general, there are more articles in February and September, then in the other months. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 7 - Show the unusual events\n",
    "\n",
    "# I will concentrate me of the higher year 2018 and show the months, which are higher in average than the rest.\n",
    "\n",
    "# Define a mask for 2018\n",
    "mask_2018 = df_plot.loc[:, 'year'] == '2018'\n",
    "df_plot_2018 = df_plot.loc[mask_2018, :]\n",
    "\n",
    "# Create barchart for monthly visualisation\n",
    "fig, ax1 = plt.subplots(figsize = (16, 10))\n",
    "graph = sns.barplot( data = df_plot_2018, x = 'month', y = 'No. of article', ci = None, estimator = sum)\n",
    "\n",
    "# Calculate the yearly average\n",
    "mean_month = df_plot_2018.groupby('month')['No. of article'].sum().mean()\n",
    "\n",
    "# Insert line with yearly average\n",
    "graph = graph.axhline(mean_month)\n",
    "\n",
    "# Insert text marker for high values and average\n",
    "ax1.text(x = 5.8, y=140, s='High above average')\n",
    "ax1.text(x = -0.25, y=116, s='Avg. line')\n",
    "\n",
    "# Insert arrows for high values\n",
    "ax1.annotate(text = '',\n",
    "xy=[7.5, 135],\n",
    "xytext=[6.8, 135],\n",
    "arrowprops=dict(facecolor='black'))\n",
    "\n",
    "ax1.annotate(text = '',\n",
    "xy=[5.5, 135],\n",
    "xytext=[6.8, 135],\n",
    "arrowprops=dict(facecolor='black'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 8 - Show the root cause of the unusual events in 2018 \n",
    "\n",
    "df_extract = df_article.reset_index()\n",
    "df_extract['webPublicationDate'] = pd.to_datetime(df_extract['webPublicationDate']).dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Extract the articles from the two months high above average June\n",
    "df_extract_june = df_extract[df_extract['webPublicationDate'].between('2018-05-31', '2018-07-01')]\n",
    "\n",
    "# Extract the articles from the two months high above average Sept\n",
    "df_extract_sept = df_extract[df_extract['webPublicationDate'].between('2018-08-31', '2018-10-01')]\n",
    "\n",
    "# Create Plot\n",
    "fig, ax2 = plt.subplots(nrows = 2, ncols = 1, figsize = (32,20))\n",
    "df_extract_june.groupby('webPublicationDate')['webTitle'].count().plot(kind = 'bar', ax = ax2[0])\n",
    "df_extract_sept.groupby('webPublicationDate')['webTitle'].count().plot(kind = 'bar', ax = ax2[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "# Define text for june\n",
    "mask_title_june = df_extract_june.loc[:, 'webTitle'] \n",
    "text_june = str(df_extract_june.loc[:, 'webTitle'])\n",
    "\n",
    "# Define text for sept\n",
    "mask_title_sept = df_extract_sept.loc[:, 'webTitle'] \n",
    "text_sept = str(df_extract_sept.loc[:, 'webTitle'])\n",
    "\n",
    "# Define unimportant words\n",
    "words_stop = \"to the of in a on ABC is and will justin trudeau for I Canada says fo - The after with like become Canadian\"\n",
    "words_stop_list = words_stop.split()\n",
    "STOPWORDS.update(words_stop_list)\n",
    "\n",
    "# Generate a word cloud image\n",
    "wordcloud_june = WordCloud(background_color=\"white\", width=1920, height=1080, min_word_length = 2, collocations=True).generate(text_june)\n",
    "wordcloud_sept = WordCloud(background_color=\"white\", width=1920, height=1080, min_word_length = 2, collocations=True).generate(text_sept)\n",
    "\n",
    "# Show \n",
    "fig, ax = plt.subplots(figsize = (16,10))\n",
    "plt.imshow(wordcloud_june, interpolation=\"bilinear\")\n",
    "#plt.imshow(wordcloud_sept, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define text for sept\n",
    "mask_title_sept = df_extract_sept.loc[:, 'webTitle'] \n",
    "text_sept = str(df_extract_sept.loc[:, 'webTitle'])\n",
    "\n",
    "# Generate a word cloud image\n",
    "wordcloud_sept = WordCloud(background_color=\"white\", width=1920, height=1080, min_word_length = 2, collocations=True).generate(text_sept)\n",
    "\n",
    "# Show \n",
    "fig, ax = plt.subplots(figsize = (16,10))\n",
    "plt.imshow(wordcloud_sept, interpolation=\"bilinear\")\n",
    "#plt.imshow(wordcloud_sept, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f08154012ddadd8e950e6e9e035c7a7b32c136e7647e9b7c77e02eb723a8bedb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
