{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HolidayCheck Case Study**\n",
    "Guardian Media API - Datenanalyse über artikel von Justin trudeau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import datetime as dt\n",
    "from datetime import date"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 1 (Abfrage)\n",
    "***\n",
    "Im ersten Schritt habe ich über das request Modul die Verbindung zur Guardian Media API hergestellt. \n",
    "\n",
    "**Challenge:**\n",
    "- Die Schnittstelle liefert nur begrenzte Ergebnisse pro Abfrage\n",
    "- Die Requests werden bei einer zu schnellen Abfragerate blockiert\n",
    "\n",
    "**Lösung:**\n",
    "- Mehrere Iterationen durch die verschiedenen Ergebnisseiten\n",
    "- Time.sleep Modul, um die Schnittstelle nicht zu überfordern\n",
    "\n",
    "**Ergebnis:**\n",
    "\n",
    "- Ein Dataframe (*df_guardian*), welches alle Publikationen seit dem 01.01.2018 enthält, welche den Namen \"Justin Trudeau\" eischliessen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1 - extract the information about Justin Trudeau since 01.01.2018\n",
    "\n",
    "url = 'https://content.guardianapis.com/search'\n",
    "search_term = 'Justin Trudeau'\n",
    "date_today = date.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "params = {'api-key':'8eb7b724-c4d4-42f0-845a-0617f9495526',\n",
    "            'page-size':50, #Page Size 100 or 200 caused an error.\n",
    "            'q':search_term,\n",
    "            'from-date': \"2018-01-01\",\n",
    "            'to-date': date_today,\n",
    "            'page': 1\n",
    "        }\n",
    "\n",
    "def query_api(url, params):\n",
    "    \"\"\"\n",
    "    Function to query the Guardian API with the preffered search term. \n",
    "    The output is a int with the amount of results and a string with the actual date and the amount of results to verify.\n",
    "    \"\"\"\n",
    "    response = requests.get(url, params)\n",
    "    return response.json()\n",
    "\n",
    "json_result = query_api(url, params)\n",
    "print('On the ' + date_today + ', we found ' + str(json_result['response']['total']) + ' results with the search term ' + search_term +'.')\n",
    "\n",
    "def query_all_articles(url, params):\n",
    "    \"\"\"\n",
    "    Function to iterate trough all the results/pages from the query and add the results to one dataframe.\n",
    "    The function adds at the params dict \"page\" in every iteration +1, to go trough every page automated. \n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(columns=['id', 'type', 'sectionId', 'sectionName', 'webPublicationDate', 'webTitle', 'webUrl', 'apiUrl','fields','isHosted','pillarId','pillarName'])\n",
    "\n",
    "    if int(json_result['response']['total']) > 200:\n",
    "        for i in range(0, (int((json_result['response']['total']) / int(params['page-size']) + 1))):\n",
    "            print('Iterate through page number ' + str(params['page'])) #To check the iterations\n",
    "            data = pd.DataFrame(query_api(url, params)['response']['results'])\n",
    "            df = pd.concat([df, data])\n",
    "            params['page'] = int(params['page']) + 1\n",
    "            time.sleep(3)\n",
    "                \n",
    "\n",
    "    return df\n",
    "\n",
    "df_guardian = query_all_articles(url, params)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 1.1 (Reinigung)\n",
    "***\n",
    "Im zweiten Schritt habe ich die Daten vor der Weiterverarbeitung gereinigt \n",
    "\n",
    "**Challenge:**\n",
    "- Wir arbeiten mit Zeitdaten (webPublicationDate)\n",
    "- Es gibt mehrere Duplikate im Datensatz, es muss entschieden werden wie wir damit fortfahren.\n",
    "- Es gibt Null / NaN Werte im Datensatz \n",
    "\n",
    "**Lösung:**\n",
    "- Die Datumsspalte (webPublicationDate) zu einem Pandas \"datetime\" format abändern\n",
    "- Die Duplikate identifizieren und entfernen, da es sich um Daten mit demselben Titel und derselben API Url handelt und diese unsere Analyse verfälschen würden. \n",
    "- Die Spalte mit den Null / NaN Werten löschen, da wir diese Spalte nicht benötigen, macht es in diesem speziellen Fall keinen Sinn ihr weitere Beachtung zu schenken. \n",
    "\n",
    "**Ergebnis:**\n",
    "\n",
    "- Ein Dataframe (*df_cleaned*) mit allen Spalten (*'type', 'sectionId', 'sectionName', 'webTitle'*), welche wir zu Beginn benötigen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check & clean the data\n",
    "df_guardian.info() \n",
    "\n",
    "# 8212 rows, 12 columns\n",
    "df_guardian.shape\n",
    "\n",
    "# All datatypes are objects --> I wil lchange the 'webPublicationDate\" to datetime format and set as index for a easier data handling\n",
    "df_guardian.dtypes\n",
    "\n",
    "df_guardian['webPublicationDate'] = pd.to_datetime(df_guardian['webPublicationDate']).dt.strftime(\"%Y-%m-%d\")\n",
    "df_guardian = df_guardian.set_index('webPublicationDate')\n",
    "\n",
    "# I have found 20 row duplicates --> I will delete it\n",
    "df_guardian[df_guardian.duplicated()].count()\n",
    "df_guardian.drop_duplicates(inplace = True)\n",
    "\n",
    "# I have found 55 duplicates with the same webTitle --> I will delete it, beacuse the most of them are \"Correction and clarification\" or Football preview articles\n",
    "dup_webtitle = df_guardian.loc[:, 'webTitle'][df_guardian.loc[:, 'webTitle'].duplicated()]\n",
    "df_guardian.drop_duplicates(subset=['webTitle'], inplace = True)\n",
    "\n",
    "# I have found 8265 NaN Values in the column \"field\" --> I don't need this column, I will delete it\n",
    "df_guardian.isna().sum()\n",
    "\n",
    "# I have found 8265 Null Values in the column \"field\" --> I don't need this column, I will delete it\n",
    "df_guardian.isnull().sum()\n",
    "\n",
    "# For the tasks, I need just the columns type, sectionID, sectionName, webPucblication, webTitle\n",
    "df_cleaned = df_guardian.loc[:, ['type', 'sectionId', 'sectionName', 'webTitle']]\n",
    "\n",
    "df_cleaned.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 2\n",
    "***\n",
    "In diesem Abschnitt habe ich analysiert, wieviele Artikel seit dem 01.01.2018 Justin Trudeau erwähnen. \n",
    "\n",
    "**Challenge:**\n",
    "- Es gibt nicht nur Artikel (*article*) sondern auch Publikationen in anderen Formaten \n",
    "\n",
    "**Lösung:**\n",
    "- Eine Boolsche Maskierung, welche in der Spalte 'type' nach dem Wert 'article' filtert.\n",
    "\n",
    "**Ergebnis:**\n",
    "- Ein Dataframe (*df_date*) welches in der Linken Spalte ein Datum und in der rechten Spalte die Anzahl an Artikel angibt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 2 - Count how many articles about Justin Trudeau have been posted since 01.01.2018 until today (The data is already in this date range), grouped by Date and \"No. of articles\".\n",
    "\n",
    "# Check how many of the types are articles\n",
    "pd.crosstab(index = df_cleaned.loc[:, 'type'],\n",
    "           columns = 'count')\n",
    "\n",
    "# Set mask for type \"article\"\n",
    "mask_article = df_cleaned.iloc[:, 0] == 'article' \n",
    "\n",
    "# Define df with the type 'article' and the date between 01.01.2018 - today\n",
    "df_article = df_cleaned.loc[mask_article, :]\n",
    "\n",
    "\n",
    "# RESULT\n",
    "\n",
    "# Create table with \"Date\" and \"No. of articles\"\n",
    "df_date = pd.crosstab(df_article.index, columns = 'No. of article', rownames=[''], colnames=['Date'])\n",
    "df_date"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 2\n",
    "***\n",
    "In dieser Aufgabe habe ich den täglichen Schnitt an Anzahl Artikel berechnet auf der Datenbasis seit 01.01.2018\n",
    "\n",
    "**Lösung:**\n",
    "- Durch die Funktion mean, kann der tägliche Durchschnitt des Dataframes df_date berechnet werden. \n",
    "\n",
    "**Ergebnis:**\n",
    "- Ein täglicher Durchschnittswert, aufgerundet. (Stand 09.01.2023: 4 Artikel pro Tag, gerundet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 3 - Calculate the average no of articles for all days for the above mentiod periods from \"No. of articles\"\n",
    "\n",
    "# RESULT\n",
    "\n",
    "# Total daily average since 01.01.2018 until today\n",
    "df_date.mean().round(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 4\n",
    "***\n",
    "In Aufgabe 4 musste ich analysieren, in welcher Sektion die meisten Artikel vorhanden sind.  \n",
    "\n",
    "**Challenge:** \n",
    "- Entscheidung, ob die \"sectionId\" oder die \"sectionName\" palte verwendet werden soll.\n",
    "\n",
    "**Lösung:**\n",
    "- Ich habe mich für die \"sectionName\" Spalte entschieden, da diese detaillierter ist.\n",
    "\n",
    "**Ergebnis:**\n",
    "- Ein Dataframe (*df_section*) bei dem sich in der linken Spalte die Sektionen befinden und in der rechten Spalte die Anzahl artikel.\n",
    "Im Ergebnis wird lediglich die Sektion, mit den meisten Artikeln ausgegeben (Stand 09.01.2022: 'World News'). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 4 - In which section (I choose 'sectionName') are the most articles written\n",
    "\n",
    "# RESULT\n",
    "\n",
    "# Define Dataframe with the sections\n",
    "df_section = pd.crosstab(index = df_article.iloc[:, 2],\n",
    "                columns = 'No. of article', \n",
    "                colnames = ['Section'], \n",
    "                rownames = ['']\n",
    ")\n",
    "\n",
    "# Show sections sorted by No. of articles (Solution: The \"World news\" section has the most articles with 1010)\n",
    "df_section.sort_values(by = ['No. of article'], ascending = False).iloc[0, :]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 5\n",
    "***\n",
    "In Aufgabe 4 musste ich analysieren, in welcher Sektion die meisten Artikel vorhanden sind.  \n",
    "\n",
    "**Challenge:** \n",
    "- Entscheidung, ob die \"sectionId\" oder die \"sectionName\" palte verwendet werden soll.\n",
    "\n",
    "**Lösung:**\n",
    "- Ich habe mich für die \"sectionName\" Spalte entschieden, da diese detaillierter ist.\n",
    "\n",
    "**Ergebnis:**\n",
    "- Ein Dataframe (*df_section*) bei dem sich in der linken Spalte die Sektionen befinden und in der rechten Spalte die Anzahl artikel.\n",
    "Im Ergebnis wird lediglich die Sektion, mit den meisten Artikeln ausgegeben (Stand 09.01.2022: 'World News'). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 5 - Show the evolution of the 'No. of articles? since 01.01.2018 until today\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# Had to change the index and add some date columns for a easier handling\n",
    "df_plot = df_date.reset_index()\n",
    "df_plot = df_plot.rename_axis(None, axis=1)\n",
    "df_plot['day'] = pd.to_datetime(df_plot['']).dt.to_period('D').dt.strftime('%d')\n",
    "df_plot['month-year'] = pd.to_datetime(df_plot['']).dt.to_period('M').dt.strftime('%Y %m')\n",
    "df_plot['month'] = pd.to_datetime(df_plot['']).dt.to_period('M').dt.strftime('%m')\n",
    "df_plot['year'] = pd.to_datetime(df_plot['']).dt.to_period('Y').dt.strftime('%Y')\n",
    "\n",
    "# Plot the data\n",
    "fig, ax = plt.subplots(nrows = 2, ncols = 1, figsize = (30, 15))\n",
    "sns.lineplot(x='month-year', y='No. of article', data=df_plot, ax=ax[0], alpha=0.8)\n",
    "sns.lineplot(x='month', y='No. of article', data=df_plot, ax=ax[1], alpha=0.8)\n",
    "\n",
    "# Style\n",
    "plt.tight_layout()\n",
    "ax[0].xaxis.set_tick_params(labelrotation=25)\n",
    "ax[1].xaxis.set_tick_params(labelrotation=25)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 6\n",
    "***\n",
    "In der Aufgabe 6 identifiziere ich unübliche Ereignisse im zeitlichen Verlauf seit 01.01.2018\n",
    "\n",
    "**Analyse:**\n",
    "\n",
    "Es gibt auf den ersten Blick auf die Visualisierung eine höhere Anzahl Artikel im Sommer / Herbst 2018 als gewöhnlich.\n",
    "\n",
    "Zusätzlich gibt es über alle Jahre hinweg gesehen, im September generell mehr Artikel als in anderen Monaten. Nach kurzen eigenen Recherchen kam ich auf den Entschluss,\n",
    "dass die Abweichungen im September daher kommen, dass in diesem Zeitraum in Kanada Wahlen stattfinden."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 7\n",
    "***\n",
    "In Aufgabe 7 zeige ich die ungewöhnlichen Ereignisse detailierter auf.  \n",
    "\n",
    "**Challenge:** \n",
    "- Entscheidung, ob die Daten mit einem Diagramm oder einer Tabelle aufgezeigt werden sollen.\n",
    "- Definierung, was ich genau darstellen möchte.\n",
    "\n",
    "**Lösung:**\n",
    "- Ich stelle die höhere Anzahl Artikel im Juni / September 2018 mit einem Balkendiagramm dar, da dieses Diagramm auf den ersten Blick die Unterschiede zu den restlichen Monaten aufzeigt.\n",
    "- Ich möchte auf eine verstänldiche Art aufzeigen, dass in den Monaten Juni und September 2018 mehr Artikel als gewöhlnich über Justin Trudeau veröffentlicht wurden.\n",
    "\n",
    "**Ergebnis:**\n",
    "- Ein Balkendiagramm mit zusätzlichen Annotationen und einer Durchschnittslinie, welches die monatlichen Artikel im Jahr 2018 darstellt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 7 - Show the unusual events\n",
    "\n",
    "# I will concentrate me of the higher year 2018 and show the months, which are higher in average than the rest.\n",
    "\n",
    "# Define a mask for 2018\n",
    "mask_2018 = df_plot.loc[:, 'year'] == '2018'\n",
    "df_plot_2018 = df_plot.loc[mask_2018, :]\n",
    "\n",
    "# Create barchart for monthly visualisation\n",
    "fig, ax1 = plt.subplots(figsize = (16, 10))\n",
    "graph = sns.barplot( data = df_plot_2018, x = 'month', y = 'No. of article', ci = None, estimator = sum)\n",
    "\n",
    "# Calculate the yearly average\n",
    "mean_month = df_plot_2018.groupby('month')['No. of article'].sum().mean()\n",
    "\n",
    "# Insert line with yearly average\n",
    "graph = graph.axhline(mean_month)\n",
    "\n",
    "# Insert text marker for high values and average\n",
    "ax1.text(x = 5.8, y=140, s='High above average')\n",
    "ax1.text(x = -0.25, y=116, s='Avg. line')\n",
    "\n",
    "# Insert arrows for high values\n",
    "ax1.annotate(text = '',\n",
    "xy=[7.5, 135],\n",
    "xytext=[6.8, 135],\n",
    "arrowprops=dict(facecolor='black'))\n",
    "\n",
    "ax1.annotate(text = '',\n",
    "xy=[5.5, 135],\n",
    "xytext=[6.8, 135],\n",
    "arrowprops=dict(facecolor='black'))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 8.1 (Tägliche Artikel)\n",
    "***\n",
    "In zeige die Ursache der erhöhten Anzahl an Artikeln auf.\n",
    "\n",
    "**Challenge:** \n",
    "- Wie kann ich die Ursache definieren\n",
    "- Wie kann ich die Ursache verständlich darstellen\n",
    "\n",
    "**Lösung:**\n",
    "- Ich extrahiere als erstes die Artikel vom Juni und vom September\n",
    "- Ich nehme die Artikeltitel um im Text mögliche Gemeinsamkeiten in den definierten Zeitperioden zu finden. Weitere Anhaltspunkte liegen mir in diesem Datensatz nicht vor, ohne weitere Quellen zu analysieren. \n",
    "- Zur Veranschaulichung habe erstelle ich zwei Balkendiagramme, welche die tägliche Artikelanzahl darstellen. Dadurch können unter Umständen Datumsspezifische Ereignisse herausgefiltert werden. \n",
    "\n",
    "**Ergebnis:**\n",
    "- Zwei Balkendiagramme, welche die tägliche Anzahl Artikel vom Juni und September 2018 darstellen.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 8 - Show the root cause of the unusual events in 2018 \n",
    "\n",
    "df_extract = df_article.reset_index()\n",
    "df_extract['webPublicationDate'] = pd.to_datetime(df_extract['webPublicationDate']).dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Extract the articles from the two months high above average June\n",
    "df_extract_june = df_extract[df_extract['webPublicationDate'].between('2018-05-31', '2018-07-01')]\n",
    "\n",
    "# Extract the articles from the two months high above average Sept\n",
    "df_extract_sept = df_extract[df_extract['webPublicationDate'].between('2018-08-31', '2018-10-01')]\n",
    "\n",
    "# Create Plot\n",
    "fig, ax2 = plt.subplots(nrows = 2, ncols = 1, figsize = (32,20))\n",
    "df_extract_june.groupby('webPublicationDate')['webTitle'].count().plot(kind = 'bar', ax = ax2[0])\n",
    "df_extract_sept.groupby('webPublicationDate')['webTitle'].count().plot(kind = 'bar', ax = ax2[1])\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 8.2 (Wortwolke)\n",
    "***\n",
    "Ich möchte analysieren, welche Themen und Wörter in dieser Periode im Titel am häufigsten vorkamen. \n",
    "\n",
    "**Challenge:** \n",
    "- Analyse des Textes, wie kann die Häufigkeit der Wörter dargestellt werden\n",
    "- Welche Schlüsse kann ich aus der Häufigkeit ziehen. \n",
    "\n",
    "**Lösung:**\n",
    "- Für die Darstellung des Textes erstelle ich eine Wortwolke, um die meist genannten Wörter auf den ersten Blick identifizieren zu können.\n",
    "- Durch die meist gennnten Wörter kann ich identifizieren, zu welchen Themen es am meisten Artikel gab. Dies kann unter Umständen die Ursache der Ausreisser sein. \n",
    "\n",
    "**Ergebnis:**\n",
    "- Zwei Wortwolken, welche die Häufigsten Wörter im Titel von den Artikeln aus Juni und September 2018 darstellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "# Define text for june\n",
    "mask_title_june = df_extract_june.loc[:, 'webTitle'] \n",
    "text_june = str(df_extract_june.loc[:, 'webTitle'])\n",
    "\n",
    "# Define text for sept\n",
    "mask_title_sept = df_extract_sept.loc[:, 'webTitle'] \n",
    "text_sept = str(df_extract_sept.loc[:, 'webTitle'])\n",
    "\n",
    "# Define unimportant words\n",
    "words_stop = \"to the of in a on ABC is and will justin trudeau for I Canada says fo - The after with like become Canadian\"\n",
    "words_stop_list = words_stop.split()\n",
    "STOPWORDS.update(words_stop_list)\n",
    "\n",
    "# Generate a word cloud image\n",
    "wordcloud_june = WordCloud(background_color=\"white\", width=1920, height=1080, min_word_length = 2, collocations=True).generate(text_june)\n",
    "wordcloud_sept = WordCloud(background_color=\"white\", width=1920, height=1080, min_word_length = 2, collocations=True).generate(text_sept)\n",
    "\n",
    "# Show \n",
    "fig, ax = plt.subplots(figsize = (16,10))\n",
    "plt.imshow(wordcloud_june, interpolation=\"bilinear\")\n",
    "#plt.imshow(wordcloud_sept, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fazit\n",
    "***\n",
    "Die Case Study war sehr interessant zu bearbeiten. Eine der grössten Challenges hatte ich gleich zu Beginn, als ich durch die verschiedenen Seiten iterieren musste, um alle Artikel zu sammeln und um die Limitierung von 200 Artikeln zu umgehen.\n",
    "\n",
    "Die Wortwolke vom Juni zeigt auf, dass viele Artikel im Zusammenhang mit Donald Trump erstellt wurden und auch der G7 Gipfel wurde einige Male genannt. Nach einigen Recherchen fand ich heraus, dass sich Donald Trump und Justing Trudeau am G7 Gipfel nicht einigen konnten und es danach von beiden seiten zu medialer Aufmerksamkeit kam. \n",
    "\n",
    "Grundsätzlich könnten die Ergebnisse anhand des Artikelinhaltes, der Kategorien oder durch hinzuzug von weiteren Quellen, natürlich noch weiter vertieft und verfeinert werden. Jedoch gibt diese Lösung eine erste Übersicht über mögliche Ursachen und es kann darauf aufgebaut werden. \n",
    "\n",
    "In der ganzen Case Study habe ich einige Programmierschritte und Kommentare absichtlich ein wenig ausführlicher gehalten, um meine Gedankengänge möglichst nachvollziehbar aufzuzeigen.   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f08154012ddadd8e950e6e9e035c7a7b32c136e7647e9b7c77e02eb723a8bedb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
