{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import schedule\n",
    "import seaborn as sns\n",
    "import requests\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import datetime as dt\n",
    "from datetime import date\n",
    "\n",
    "def daily_job():\n",
    "\n",
    "    url = 'https://content.guardianapis.com/search'\n",
    "    search_term = 'Justin Trudeau'\n",
    "    date_today = date.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    params = {'api-key': 'test',\n",
    "                'page-size': 50,  # Page Size 100 or 200 caused an error.\n",
    "                'q': search_term,\n",
    "                'from-date': \"2018-01-01\",\n",
    "                'to-date': date_today,\n",
    "                'page': 1\n",
    "                }\n",
    "\n",
    "    def query_api(url, params):\n",
    "            \"\"\"\n",
    "            Function to query the Guardian API with the preffered search term.\n",
    "            The output is a int with the amount of results and a string with the actual date and the amount of results to verify.\n",
    "            \"\"\"\n",
    "            response = requests.get(url, params)\n",
    "            return response.json()\n",
    "\n",
    "    json_result = query_api(url, params)\n",
    "\n",
    "    def query_all_articles(url, params):\n",
    "            \"\"\"\n",
    "            Function to iterate trough all the results/pages from the query and add the results to one dataframe.\n",
    "            The function adds at the params dict \"page\" in every iteration +1, to go trough every page automated. \n",
    "            \"\"\"\n",
    "            df = pd.DataFrame(columns=['id', 'type', 'sectionId', 'sectionName', 'webPublicationDate',\n",
    "                            'webTitle', 'webUrl', 'apiUrl', 'fields', 'isHosted', 'pillarId', 'pillarName'])\n",
    "\n",
    "            if int(json_result['response']['total']) > 200:\n",
    "                for i in range(0, (int((json_result['response']['total']) / int(params['page-size']) + 1))):\n",
    "                    # To check the iterations\n",
    "                    print('Iterate through page number ' + str(params['page']))\n",
    "                    data = pd.DataFrame(query_api(url, params)[\n",
    "                                        'response']['results'])\n",
    "                    df = pd.concat([df, data])\n",
    "                    params['page'] = int(params['page']) + 1\n",
    "                    time.sleep(3)\n",
    "\n",
    "            return df\n",
    "\n",
    "    df_guardian = query_all_articles(url, params)\n",
    "\n",
    "    # Data cleaning\n",
    "    df_guardian['webPublicationDate'] = pd.to_datetime(df_guardian['webPublicationDate']).dt.strftime(\"%Y-%m-%d\")\n",
    "    df_guardian = df_guardian.set_index('webPublicationDate')\n",
    "    df_guardian.drop_duplicates(inplace=True)\n",
    "    df_guardian.drop_duplicates(subset=['webTitle'], inplace=True)\n",
    "\n",
    "    df_cleaned = df_guardian.loc[:, ['type', 'sectionId', 'sectionName', 'webTitle']]\n",
    "\n",
    "    # Set mask for type \"article\"\n",
    "    mask_article = df_cleaned.iloc[:, 0] == 'article'\n",
    "\n",
    "    # Define df with the type 'article' and the date between 01.01.2018 - today\n",
    "    df_article = df_cleaned.loc[mask_article, :]\n",
    "\n",
    "    # Create table with \"Date\" and \"No. of articles\"\n",
    "    df_date = pd.crosstab(df_article.index, columns='No. of article', rownames=[''], colnames=['Date'])\n",
    "\n",
    "    # Had to change the index and add some date columns for a easier handling\n",
    "    df_plot = df_date.reset_index()\n",
    "    df_plot = df_plot.rename_axis(None, axis=1)\n",
    "    df_plot['month-year'] = pd.to_datetime(df_plot['']).dt.to_period('M').dt.strftime('%Y %m')\n",
    "    df_plot['month'] = pd.to_datetime(df_plot['']).dt.to_period('M').dt.strftime('%m')\n",
    "\n",
    "    # Plot the data\n",
    "    fig, ax = plt.subplots(figsize=(16, 10))\n",
    "    sns.lineplot(x='month-year', y='No. of article', data=df_plot, alpha=0.8)\n",
    "\n",
    "    # Style\n",
    "    fig.suptitle('Guardian Media article Evolution about Justin Trudeau since 01.01.2018', fontsize = 24)\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('No. of article', fontsize = 16)\n",
    "    ax.xaxis.set_tick_params(labelrotation=25)\n",
    "    ax.set_xticks(ax.get_xticks()[::2])\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig('Evolution_'+str(date_today)+'.pdf', format='pdf')\n",
    "\n",
    "\n",
    "schedule.every().day.at(\"02:00\").do(daily_job)\n",
    "\n",
    "while True:\n",
    "        schedule.run_pending()\n",
    "        time.sleep(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daily Job\n",
    "***\n",
    "Durch einen Daily Job, möchte ich das Diagramm aus Aufgabe 5 täglich aktualisieren und aufbereiten, damit es an ausstenehende Personen gesendet werden kann. \n",
    "\n",
    "**Challenge:** \n",
    "- Welche Methode verwendet ich für den Daily Job\n",
    "- Wie stelle ich das Diagramm dar um es zu exportieren\n",
    "\n",
    "**Lösung:**\n",
    "- Ich verwende Python scheduler Modul um das Skript jede Nacht um 02:00 auszuführen.\n",
    "- Ich erstelle ein automatisiertes PDF mit dem Diagramm, welches automatisch mit dem tagesaktuellen Datum abgelegt wird und versendet werden kann.  \n",
    "\n",
    "**Ergebnis:**\n",
    "- Das komplett benötigte Skript inkl. den Scheduler\n",
    "- Ein PDF Export (Evolution_yyy-mm-dd) mit dem Diagramm "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fazit\n",
    "***\n",
    "\n",
    "Es würde in diesem Fall auch andere Lösungen geben. Lokal könnte zb. mit cron Jobs oder dem Windows scheduler gearbeitet werden, um das Skript auszuführen (nicht unbedingt optimal für eine Business Umgebung). Für mich wäre jedoch die optimalste Lösung, das Python File auf einem Server laufen zu lassen und die Daten direkt in eine Datenbank zu übertragen, von welcher dann Reports oder Dashboards erstellt werden können. Für diesen Case Study Fall habe ich ebenfalls ein Jupyter Notebook erstellt, da ich somit mehr Kontext geben kann als bei einem reinen Python File. \n",
    "\n",
    "Für das PDF Diagramm gibt es natürlich noch viele weiter Layoutoptionen. Ich habe mich lediglich für eine Basisvariante entschieden, welche durch den Diagrammtitel den Inhalt beschreibt. Ein Beispiel ist ebenfalls im Guardian Ordner abgelegt.\n",
    "\n",
    "Ganz grundsätzlich, würde ich in der Praxis keinen regelmässigen Report versenden, welcher die Daten über drei Jahre in dieser Form darstellt, da die vielen Datumsangaben doch eher unübersichtlich sind. Aus diesem Grund stelle ich auch nur jeden zweiten Monat auf der Achse dar. Für eine erste ad-hoc Analyse und eine kurze Übersicht ist es in meinen Augen jedoch passend. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f08154012ddadd8e950e6e9e035c7a7b32c136e7647e9b7c77e02eb723a8bedb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
